[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "Introduction\nIn this file we will be exploring models to properly fit the diabetes data. The techniques we will be using are logistic regression, classification, and random forest. Using these fits we will evaluate the fit of each method and decide on the best one. As insight we use our EDA to help us make models. Below is a link to EDA: #Link EDA Click here for the EDA Page\n#Log-loss To create models we are going to use log-loss as a metric to evaluate. Upon research into the log-loss metric, it is defined as, “Log-loss is indicative of how close the prediction probability is to the corresponding actual/true value (0 or 1 in case of binary classification). The more the predicted probability diverges from the actual value, the higher is the log-loss value.” (Dembla, 2020) For a singular log-loss value use the equation: -(yln(p)+(1-y)ln(1-p)). Where y is a given observed value and p is the prediction probability of that observed value. By taking the sum over the sample size and divide by the sample size we observe our log-loss metric for the model. A perfect model has a 0 log-loss score and the most unsuitable model has an ifinite log-loss score. We prefer this when using a binary response variable because,“Unlike other metrics such as accuracy, Log Loss takes into account the uncertainty of predictions by penalizing models more heavily for confidently incorrect predictions.” (DataScience-ProF, 2024) Information for this section was developed using https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a and https://medium.com/@TheDataScience-ProF/understanding-log-loss-a-comprehensive-guide-with-code-examples-c79cf5411426.\n#Split data In this section we will partition our data into a train and test set using 70% and 30%, respectfully.\n\n#read in library and set seed for reproducibility\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(Metrics)\n\n\nAttaching package: 'Metrics'\n\n\nThe following objects are masked from 'package:caret':\n\n    precision, recall\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ purrr::lift()   masks caret::lift()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(15)\n\n#read in data\ndiabetes_binary_health_indicators_BRFSS2015_csv &lt;- read_csv(\"C:/Users/beard/Downloads/diabetes_binary_health_indicators_BRFSS2015.csv.zip\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#select predictors and convert factors\ndiabetes&lt;-diabetes_binary_health_indicators_BRFSS2015_csv |&gt;\n  select(c(Diabetes_binary,HighBP,HighChol,Smoker,Stroke,PhysActivity,HvyAlcoholConsump,Sex,Age,Income,BMI)) |&gt;\n  mutate( , Diabetes_binary=factor(Diabetes_binary, levels=c(0,1), labels= c(\"No\",\"Pre\")),\n          HighBP=factor(HighBP, levels= c(0,1), labels=c(\"No\",\"Yes\")),\n          HighChol=factor(HighChol, levels= c(0,1), labels=c(\"No\",\"Yes\")),\n          Smoker=factor(Smoker, levels= c(0,1), labels=c(\"No\",\"Yes\")),\n          Stroke=factor(Stroke, levels= c(0,1), labels=c(\"No\",\"Yes\")),\n          PhysActivity=factor(PhysActivity, levels= c(0,1), labels=c(\"No\",\"Yes\")),\n          HvyAlcoholConsump=factor(HvyAlcoholConsump, levels= c(0,1), labels=c(\"No\",\"Yes\")),\n          Sex=factor(Sex, levels= c(0,1), labels=c(\"Female\",\"Male\")),\n          Age=factor(Age, levels= 1:14, labels=c(\"18-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\"50-54\",\"55-59\",\"60-64\",\"65-69\",\"70-74\",\"75-79\",\"80+\",\"Unkown/Missing\")),\n          Income=factor(Income, levels= c(0:8,77,99, NA_real_), labels=c(\"&gt;$10,000\",\"$10,000-15,000\",\"$15,000-20,000\",\"$20,000-25,000\",\"$25,000-35,000\",\"$35,000-50,000\",\"$50,000-75,000\",\"$75,000+\",\"Unkown\",\"Refused\",\"Missing\"))) \n\n#Split data\nsplit &lt;- createDataPartition(y = diabetes$Diabetes_binary, p = 0.7, list = FALSE)\ntrain &lt;- data.frame(diabetes[split, ])\ntest &lt;- data.frame(diabetes[-split, ])\n\n#Logistic Regression Logistic regression is a model that is helpful when you have a binary response variables, therefore a proper method to use here. Logistic regression models use the probability of success to model the data. We will investigate the probability of having pre-diabetes using three different models.\n\n#First model: all predictors\nlr_fit1&lt;-train(Diabetes_binary~.,\n               data=train,\n               method=\"glm\",\n               family=\"binomial\",\n               metric=\"logLoss\",\n               trControl=trainControl(method=\"cv\",\n                                      number=5,\n                                      summaryFunction = mnLogLoss,\n                                      classProb=TRUE))\n\nlr_fit1\n\nGeneralized Linear Model \n\n177577 samples\n    10 predictor\n     2 classes: 'No', 'Pre' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142061, 142063, 142061, 142062 \nResampling results:\n\n  logLoss \n  0.332295\n\nlr_fit1_ll&lt;-lr_fit1$results[\"logLoss\"]\n\nlr_fit2&lt;-train(Diabetes_binary~.+Age:Sex,\n               data=train,\n               method=\"glm\",\n               family=\"binomial\",\n               metric=\"logLoss\",\n               trControl=trainControl(method=\"cv\",\n                                      number=5,\n                                      summaryFunction = mnLogLoss,\n                                      classProb=TRUE))\n\nlr_fit2\n\nGeneralized Linear Model \n\n177577 samples\n    10 predictor\n     2 classes: 'No', 'Pre' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142062, 142061, 142062, 142061 \nResampling results:\n\n  logLoss  \n  0.3319601\n\nlr_fit2_ll&lt;-lr_fit2$results[\"logLoss\"]\n\nlr_fit3&lt;-train(Diabetes_binary~.+BMI:PhysActivity,\n               data=train,\n               method=\"glm\",\n               family=\"binomial\",\n               metric=\"logLoss\",\n               trControl=trainControl(method=\"cv\",\n                                      number=5,\n                                      summaryFunction = mnLogLoss,\n                                      classProb=TRUE))\n\nlr_fit3\n\nGeneralized Linear Model \n\n177577 samples\n    10 predictor\n     2 classes: 'No', 'Pre' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142061, 142063, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3321879\n\nlr_fit3_ll&lt;-lr_fit3$results[\"logLoss\"]\n\n#create data frame to visually compare\nlr_fits_ll&lt;-cbind(lr_fit1_ll,lr_fit2_ll,lr_fit3_ll)\nlr_fits_ll\n\n   logLoss   logLoss   logLoss\n1 0.332295 0.3319601 0.3321879\n\n#Model 2 has the lowest Log loss we will now denote it as best model\nlr_best&lt;-lr_fit2\n\n#Classification Trees Classification trees are method to classify a group. This is fitting here as we can use it to classify No or pre-diabetes.\n\nct_fit&lt;-train(Diabetes_binary~.,\n              data = train,\n              method = \"rpart\",\n              metric=\"logLoss\",\n              trControl = trainControl(method = \"cv\",\n              number = 5,\n              summaryFunction = mnLogLoss,\n              classProb=TRUE),\n              preProcess = c(\"center\", \"scale\"),\n              tuneGrid = data.frame(cp = seq(0, 0.1, 0.01)))\n\nct_fit\n\nCART \n\n177577 samples\n    10 predictor\n     2 classes: 'No', 'Pre' \n\nPre-processing: centered (31), scaled (31) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142062, 142062, 142061, 142061 \nResampling results across tuning parameters:\n\n  cp    logLoss  \n  0.00  0.3653591\n  0.01  0.4037576\n  0.02  0.4037576\n  0.03  0.4037576\n  0.04  0.4037576\n  0.05  0.4037576\n  0.06  0.4037576\n  0.07  0.4037576\n  0.08  0.4037576\n  0.09  0.4037576\n  0.10  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\n\n\nRandom Forest\nRandom forest is a method of classification tree but using an average of bootstrap sample’s trees but each tree uses a random subset of predictors. It is applicable here because we have lots of predictors for the binary response variable.\n\nrf_fit&lt;-train(Diabetes_binary~.,\n              data = train,\n              method = \"rf\",\n              metric= \"logLoss\",\n              ntree=100,\n              trControl = trainControl(method = \"cv\",\n              number = 3,\n              summaryFunction = mnLogLoss,\n              classProb=TRUE),\n              preProcess = c(\"center\", \"scale\"),\n              tuneGrid = data.frame(mtry = 1:8))\n\nrf_fit\n\nRandom Forest \n\n177577 samples\n    10 predictor\n     2 classes: 'No', 'Pre' \n\nPre-processing: centered (31), scaled (31) \nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 118384, 118386, 118384 \nResampling results across tuning parameters:\n\n  mtry  logLoss \n  1     4.430960\n  2     3.309868\n  3     2.480010\n  4     1.986324\n  5     1.680403\n  6     1.503868\n  7     1.387732\n  8     1.299227\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 8.\n\n\n\n\nModel Comparison\nWe now have three best models and will be fitting them under the test set to determine the best model overall.\n\n#Logistic regression\nlr_pred_class&lt;-predict(lr_best, newdata = test)\nlr_pred_prob&lt;-predict(lr_best, newdata=test, type = \"prob\") \nlr_pred &lt;- numeric(length(lr_pred_class))\nfor (i in seq_along(lr_pred_class)) {\n  if (lr_pred_class[i] == \"No\") {\n    lr_pred[i] &lt;- lr_pred_prob$No[i]\n  } else if (lr_pred_class[i] == \"Pre\") {\n    lr_pred[i] &lt;- lr_pred_prob$Pre[i]\n  } \n}\n\nlr_actual &lt;- numeric(length(test$Diabetes_binary))\nfor (i in seq_along(test$Diabetes_binary)) {\n  if (test$Diabetes_binary[i] == \"No\") {\n    lr_actual[i] &lt;- lr_pred_prob$No[i]\n  } else if (test$Diabetes_binary[i] == \"Pre\") {\n    lr_actual[i] &lt;- lr_pred_prob$Pre[i]\n  } \n}\n\nlr_test_ll&lt;-logLoss(lr_actual,lr_pred)\nlr_test_ll\n\n[1] 0.1876736\n\n#Classification tree\nct_pred_class&lt;-predict(ct_fit, newdata = test)\nct_pred_prob&lt;-predict(ct_fit, newdata=test, type = \"prob\") \nct_pred &lt;- numeric(length(ct_pred_class))\nfor (i in seq_along(ct_pred_class)) {\n  if (ct_pred_class[i] == \"No\") {\n    ct_pred[i] &lt;- ct_pred_prob$No[i]\n  } else if (ct_pred_class[i] == \"Pre\") {\n    ct_pred[i] &lt;- ct_pred_prob$Pre[i]\n  } \n}\n\nct_actual &lt;- numeric(length(test$Diabetes_binary))\nfor (i in seq_along(test$Diabetes_binary)) {\n  if (test$Diabetes_binary[i] == \"No\") {\n    ct_actual[i] &lt;- ct_pred_prob$No[i]\n  } else if (test$Diabetes_binary[i] == \"Pre\") {\n    ct_actual[i] &lt;- ct_pred_prob$Pre[i]\n  } \n}\n\nct_test_ll&lt;-logLoss(ct_actual,ct_pred)\nct_test_ll\n\n[1] Inf\n\n#Random forest\nrf_pred_class&lt;-predict(rf_fit, newdata = test)\nrf_pred_prob&lt;-predict(rf_fit, newdata=test, type = \"prob\") \nrf_pred &lt;- numeric(length(rf_pred_class))\nfor (i in seq_along(rf_pred_class)) {\n  if (rf_pred_class[i] == \"No\") {\n    rf_pred[i] &lt;- rf_pred_prob$No[i]\n  } else if (rf_pred_class[i] == \"Pre\") {\n    rf_pred[i] &lt;- rf_pred_prob$Pre[i]\n  } \n}\n\nrf_actual &lt;- numeric(length(test$Diabetes_binary))\nfor (i in seq_along(test$Diabetes_binary)) {\n  if (test$Diabetes_binary[i] == \"No\") {\n    rf_actual[i] &lt;- rf_pred_prob$No[i]\n  } else if (test$Diabetes_binary[i] == \"Pre\") {\n    rf_actual[i] &lt;- rf_pred_prob$Pre[i]\n  } \n}\n\nrf_test_ll&lt;-logLoss(rf_actual,rf_pred)\nrf_test_ll\n\n[1] Inf\n\ntest_ll&lt;-cbind(\"logistic regression\"=lr_test_ll, \"classification tree\"=ct_test_ll, \"random forest\"=rf_test_ll)\ntest_ll\n\n     logistic regression classification tree random forest\n[1,]           0.1876736                 Inf           Inf\n\n\n\n\nLog loss\nWith the lowest log-loss of .1877, logistic regression is the best model for this data set.\n\nbest_model&lt;-lr_best"
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "Introduction\nIn this file I will be conducting an exploratory data analysis to better understand the data set. The data set evaluates if someone has diabetes and tracks there risk factors such as Age, BMI, and other health conditions. In this EDA I will explore numeric summaries and distributions of variables and variables in respect to the response variable and each other. Discovering this will help us fit a model for prediction.\n\n\nData Cleaning\nFirst, read in the data. Then, to properly use the data we need to convert variables that get read in as numeric as factors.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n#read in data\ndiabetes_binary_health_indicators_BRFSS2015_csv &lt;- read_csv(\"C:/Users/beard/Downloads/diabetes_binary_health_indicators_BRFSS2015.csv.zip\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#select predictors and convert factors\ndiabetes&lt;-diabetes_binary_health_indicators_BRFSS2015_csv |&gt;\n  select(c(Diabetes_binary,HighBP,HighChol,Smoker,Stroke,PhysActivity,HvyAlcoholConsump,Sex,Age,Income,BMI)) |&gt;\n  mutate( , Diabetes_binary=factor(Diabetes_binary, levels=c(0,1), labels= c(\"No\",\"Pre\")),\n          HighBP=factor(HighBP, levels= c(0,1), labels=c(\"No\",\"Yes\")),\n          HighChol=factor(HighChol, levels= c(0,1), labels=c(\"No\",\"Yes\")),\n          Smoker=factor(Smoker, levels= c(0,1), labels=c(\"No\",\"Yes\")),\n          Stroke=factor(Stroke, levels= c(0,1), labels=c(\"No\",\"Yes\")),\n          PhysActivity=factor(PhysActivity, levels= c(0,1), labels=c(\"No\",\"Yes\")),\n          HvyAlcoholConsump=factor(HvyAlcoholConsump, levels= c(0,1), labels=c(\"No\",\"Yes\")),\n          Sex=factor(Sex, levels= c(0,1), labels=c(\"Female\",\"Male\")),\n          Age=factor(Age, levels= 1:14, labels=c(\"18-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\"50-54\",\"55-59\",\"60-64\",\"65-69\",\"70-74\",\"75-79\",\"80+\",\"Unkown/Missing\")),\n          Income=factor(Income, levels= c(0:8,77,99, NA_real_), labels=c(\"&gt;$10,000\",\"$10,000-15,000\",\"$15,000-20,000\",\"$20,000-25,000\",\"$25,000-35,000\",\"$35,000-50,000\",\"$50,000-75,000\",\"$75,000+\",\"Unkown\",\"Refused\",\"Missing\"))) \n\nhead(diabetes)\n\n# A tibble: 6 × 11\n  Diabetes_binary HighBP HighChol Smoker Stroke PhysActivity HvyAlcoholConsump\n  &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;        &lt;fct&gt;            \n1 No              Yes    Yes      Yes    No     No           No               \n2 No              No     No       Yes    No     Yes          No               \n3 No              Yes    Yes      No     No     No           No               \n4 No              Yes    No       No     No     Yes          No               \n5 No              Yes    Yes      No     No     Yes          No               \n6 No              Yes    Yes      Yes    No     Yes          No               \n# ℹ 4 more variables: Sex &lt;fct&gt;, Age &lt;fct&gt;, Income &lt;fct&gt;, BMI &lt;dbl&gt;\n\n\n\n\nCheck Missingness\n\ncolSums(is.na(diabetes))\n\n  Diabetes_binary            HighBP          HighChol            Smoker \n                0                 0                 0                 0 \n           Stroke      PhysActivity HvyAlcoholConsump               Sex \n                0                 0                 0                 0 \n              Age            Income               BMI \n                0                 0                 0 \n\n\n\n\nNumeric Summeries\nEvaluate numeric variables with numeric summaries. Plot histogram of numeric variables. Plots of numeric variables factored by Diabetes Status.\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\nlibrary(ggplot2)\n\n#numeric summary\ndescribe(diabetes$BMI)\n\n   vars      n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 253680 28.38 6.61     27   27.68 4.45  12  98    86 2.12       11 0.01\n\n#plots of numeric predictors\nggplot(diabetes, aes(x=BMI)) +\n  geom_histogram(fill=\"green\") +\n  labs(x=\"BMI\",\n       y=\"Count\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n#plots of numeric predictors factored by Diabetes Status\nggplot(diabetes, aes(x=BMI, fill=Diabetes_binary)) +\n  geom_density()+\n  labs(x=\"BMI\",\n       y=\"Density\")+\n  scale_fill_manual(values=c(\"No\"=\"green\", \"Pre\"=\"pink\"), name=\"Diabetes Status\")\n\n\n\n\n\n\n\n\n\n\nFrequencies\nCreation of bar charts for categorical variables with frequencies layered on top. Note the y-axis denotes counts.\n\ndiabetes_chart&lt;-ggplot(diabetes, aes(x=Diabetes_binary)) +\n  geom_bar(fill=\"green\")+\n  geom_text(stat = \"count\", aes(label=paste0(round(..count../sum(..count..)*100,1),\"%\")), vjust=-.1)+\n  labs(x=\"Diabetes Status\",\n       y=\"Frequencies\")\ndiabetes_chart\n\n\n\n\n\n\n\nHighBP_chart&lt;-ggplot(diabetes, aes(x=HighBP)) +\n  geom_bar(fill=\"green\")+\n  geom_text(stat = \"count\", aes(label=paste0(round(..count../sum(..count..)*100,1),\"%\")),vjust=-.05)+\n  labs(x=\"High BP\",\n       y=\"Frequencies\")\nHighBP_chart\n\n\n\n\n\n\n\nHighChol_chart&lt;-ggplot(diabetes, aes(x=HighChol)) +\n  geom_bar(fill=\"green\")+\n  geom_text(stat = \"count\", aes(label=paste0(round(..count../sum(..count..)*100,1),\"%\")),vjust=-.05)+\n  labs(x=\"High Cholestrol\",\n       y=\"Frequencies\")\nHighChol_chart\n\n\n\n\n\n\n\nSmoker_chart&lt;-ggplot(diabetes, aes(x=Smoker)) +\n  geom_bar(fill=\"green\")+\n  geom_text(stat = \"count\", aes(label=paste0(round(..count../sum(..count..)*100,1),\"%\")),vjust=-.05)+\n  labs(x=\"Smoker\",\n       y=\"Frequencies\")\nSmoker_chart\n\n\n\n\n\n\n\nStroke_chart&lt;-ggplot(diabetes, aes(x=Stroke)) +\n  geom_bar(fill=\"green\")+\n  geom_text(stat = \"count\", aes(label=paste0(round(..count../sum(..count..)*100,1),\"%\")),vjust=-.05)+\n  labs(x=\"Stroke\",\n       y=\"Frequencies\")\nStroke_chart\n\n\n\n\n\n\n\nActivity_chart&lt;-ggplot(diabetes, aes(x=PhysActivity)) +\n  geom_bar(fill=\"green\")+\n  geom_text(stat = \"count\", aes(label=paste0(round(..count../sum(..count..)*100,1),\"%\")),vjust=-.05)+\n  labs(x=\"Physical Activity in past 30 days\",\n       y=\"Frequencies\")\nActivity_chart\n\n\n\n\n\n\n\nAlcohol_chart&lt;-ggplot(diabetes, aes(x=HvyAlcoholConsump)) +\n  geom_bar(fill=\"green\")+\n  geom_text(stat = \"count\", aes(label=paste0(round(..count../sum(..count..)*100,1),\"%\")),vjust=-.05)+\n  labs(x=\"Heavy drinking\",\n       y=\"Frequencies\")\nAlcohol_chart\n\n\n\n\n\n\n\nSex_chart&lt;-ggplot(diabetes, aes(x=Sex)) +\n  geom_bar(fill=\"green\")+\n  geom_text(stat = \"count\", aes(label=paste0(round(..count../sum(..count..)*100,1),\"%\")),vjust=-.05)+\n  labs(x=\"Sex\",\n       y=\"Frequencies\")\nSex_chart\n\n\n\n\n\n\n\nAge_chart&lt;-ggplot(diabetes, aes(x=Age)) +\n  geom_bar(fill=\"green\")+\n  geom_text(stat = \"count\", aes(label=paste0(round(..count../sum(..count..)*100,1),\"%\")),vjust=-.05)+\n  labs(x=\"Age\",\n       y=\"Frequencies\")\nAge_chart\n\n\n\n\n\n\n\nIncome_chart&lt;-ggplot(diabetes, aes(x=Income)) +\n  geom_bar(fill=\"green\")+\n  geom_text(stat = \"count\", aes(label=paste0(round(..count../sum(..count..)*100,1),\"%\")),vjust=-.05)+\n  labs(x=\"Income\",\n       y=\"Frequencies\") +\n  theme(axis.text.x = element_text(size = 6))\nIncome_chart\n\n\n\n\n\n\n\n\n\n\nContingency tables\nCreate contingency tables with categorical predictors against Diabetes Status.\n\ndiabetes |&gt;\n  drop_na(Diabetes_binary, HighBP) |&gt;\n  group_by(Diabetes_binary,HighBP) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = HighBP, values_from = count)\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary     No   Yes\n  &lt;fct&gt;            &lt;int&gt; &lt;int&gt;\n1 No              136109 82225\n2 Pre               8742 26604\n\ndiabetes |&gt;\n  drop_na(Diabetes_binary, HighChol) |&gt;\n  group_by(Diabetes_binary,HighChol) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = HighChol, values_from = count)\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary     No   Yes\n  &lt;fct&gt;            &lt;int&gt; &lt;int&gt;\n1 No              134429 83905\n2 Pre              11660 23686\n\ndiabetes |&gt;\n  drop_na(Diabetes_binary, Smoker) |&gt;\n  group_by(Diabetes_binary,Smoker) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Smoker, values_from = count)\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary     No   Yes\n  &lt;fct&gt;            &lt;int&gt; &lt;int&gt;\n1 No              124228 94106\n2 Pre              17029 18317\n\ndiabetes |&gt;\n  drop_na(Diabetes_binary, Stroke) |&gt;\n  group_by(Diabetes_binary,Stroke) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Stroke, values_from = count)\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary     No   Yes\n  &lt;fct&gt;            &lt;int&gt; &lt;int&gt;\n1 No              211310  7024\n2 Pre              32078  3268\n\ndiabetes |&gt;\n  drop_na(Diabetes_binary, PhysActivity) |&gt;\n  group_by(Diabetes_binary,PhysActivity) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = PhysActivity, values_from = count)\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary    No    Yes\n  &lt;fct&gt;           &lt;int&gt;  &lt;int&gt;\n1 No              48701 169633\n2 Pre             13059  22287\n\ndiabetes |&gt;\n  drop_na(Diabetes_binary, HvyAlcoholConsump) |&gt;\n  group_by(Diabetes_binary,HvyAlcoholConsump) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = HvyAlcoholConsump, values_from = count)\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary     No   Yes\n  &lt;fct&gt;            &lt;int&gt; &lt;int&gt;\n1 No              204910 13424\n2 Pre              34514   832\n\ndiabetes |&gt;\n  drop_na(Diabetes_binary, Sex) |&gt;\n  group_by(Diabetes_binary,Sex) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Sex, values_from = count)\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary Female  Male\n  &lt;fct&gt;            &lt;int&gt; &lt;int&gt;\n1 No              123563 94771\n2 Pre              18411 16935\n\ndiabetes |&gt;\n  drop_na(Diabetes_binary, Age) |&gt;\n  group_by(Diabetes_binary,Age) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Age, values_from = count)\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 2 × 14\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary `18-24` `25-29` `30-34` `35-39` `40-44` `45-49` `50-54`\n  &lt;fct&gt;             &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n1 No                 5622    7458   10809   13197   15106   18077   23226\n2 Pre                  78     140     314     626    1051    1742    3088\n# ℹ 6 more variables: `55-59` &lt;int&gt;, `60-64` &lt;int&gt;, `65-69` &lt;int&gt;,\n#   `70-74` &lt;int&gt;, `75-79` &lt;int&gt;, `80+` &lt;int&gt;\n\ndiabetes |&gt;\n  drop_na(Diabetes_binary, Income) |&gt;\n  group_by(Diabetes_binary,Income) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Income, values_from = count)\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 2 × 9\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary `$10,000-15,000` `$15,000-20,000` `$20,000-25,000`\n  &lt;fct&gt;                      &lt;int&gt;            &lt;int&gt;            &lt;int&gt;\n1 No                          7428             8697            12426\n2 Pre                         2383             3086             3568\n# ℹ 5 more variables: `$25,000-35,000` &lt;int&gt;, `$35,000-50,000` &lt;int&gt;,\n#   `$50,000-75,000` &lt;int&gt;, `$75,000+` &lt;int&gt;, Unkown &lt;int&gt;\n\n\n#Link to modeling hmtl Click here for the Modeling Page"
  }
]